## 文件结构
1. ./oldmir: 没什么用
2. ./validator: 没什么用，用于验证签名的，但是因为实验中不会伪造签名，所以没有实现任何功能
3. ./util: 一个关于使用channel作为输入输出缓存的工具函数，具体说，实现了一个带有Buffer的输入输出通道，同时带有一个周期启动某函数的功能
4. ./config: 一个读取配置的文件，然后在这里可以设置Log等级
5. ./crypto: 保存了在区块链中常用的加密工具
6. ./tracing: 一个汇集各类event以及追踪记录其方法的工具，估计只是为了追踪某些事件设计的
7. ./tsl-data: 身份信息
8. ./profiling: 性能分析工具，追踪cpu并记录为文件
9. ./discovery： 客户和节点之间互相发现的方法
    * client 有关于注册和发现服务器的方法
    * server 有启动服务器和发现其他节点的方法
    * rpcs 定义了通信协议
    * master 是接受命令并指示server调整配置的文件
10. ./cmd 盲猜是用于启动各个模块的代码，如启动发现服务，启动client，启动server等等
## 实际需要关注的
1. ./log:
    * 区块链的结构是： 区块是一个个Entry，而Log则为链本身
    * entry： 定义了区块的结构，其中Suspect估计是用于黑名单
    * log： 描述了区块链日志的基本功能，如提交区块，调出区块，以及用于其他节点跟进CatchUp的发布和订阅区块
2. ./request: 
    * 客户的请求先进入Bucket中，然后由BucketGroup剪切成Batch
    *  bucket 实现了一个带有锁的双向链表结构，用于添加或移除事务
    *  bucketGroup 实现了了从Bucket到Batch的一步，具体说就是根据特定的定时或者数量按时取出Req组织成Batch
    *  request 描述了Req进入共识前的方法，其中有Req的Buffer和Bucket的分配，Req跟随Watermark的移动，Req的哈希计算等等
    *  buffer 似乎是Client自己的Buffer 用于记录和缓存自己的Req 其中包含了缓存未到使用的Req 推进WaterMark并取出Backlog
    *  batch 是Client的信息到来的第一部处理，用于制作成Batch或者将Batch还原成CLientReqMsg，还包括一些检测回收没有被提议的Req，验证哈希等附加功能
    *  requesthandler 用于存放Req，多个线程处理
    *  responder 监听logChan并响应信息
3. ./statetransfer: 一个处理missing entry的包，包括描述节点如何发起catch up和如何响应该request，之所以叫做这个名词，估计本义是说该文件用于同步状态进度
4. ./manger: 
    * manger是管理各个模块交互的部分，如如何分配Seg，如何让节点并行执行共识，如何实现跟进同步等等，不过不清楚这个是每个节点均有的模块还是中心的模块
    * segment： 论文中所说的Seg
    * contiguous segment: 就是论文中的Seg结构
    * skipping segment： 应该并非是作者文中示范的Seg结构，他的seg中的SN可以跳跃
    * leader policies: 用于选领导节点，提供了以下的方法： Simple(所有节点均为leader) Single(按轮转法根据epoch和len(node)选出) Backoff（根据配置直接禁用某些节点） Blacklist(根据节点提供的黑名单禁用某些节点)
    * manger: 一个接口
    * mir manger: 真正的manger，他主要负责两件事情： 1. 处理Entry,从本地节点的Log中不断分析entry，根据是否Aborted更新黑名单，根据Sn判断是否新的Epoch并初始化该Epoch的一些变量，签发新的Seg（根据规则划分出Seg，找到自己的Seg并确认桶的分配） 2. 监听新的Checkpoint并在落后时跟进
    * dummy manger： 还不用看

4. ./protobufs: 保存了该项目所使用的数据结构：
5. ./membership: 一个描述节点验证信息的文件，用于初始化client和node的密钥身份
6. ./orderer： 具体的共识协议
    * common: 实现了一个backlog，应该是在共识始前缓存工作，具体说有： 在新的协议信息到来，则缓存到backlog ； 在新的订阅者到来，将backlog拷贝一份； 在新的epoch到来，清理旧的backlog垃圾回收
    * orderer: 通用的排序器接口，start让协议开始 HandleMessage描述如何处理各种协议信息 HandleEntry描述共识结束的提交步骤 Sign签名信息池check sig检查签名
    * instance: 共识实例
    * backlog: 共识的backlog
    * orderer: 共识的orderer
    * orderer和backlog实现整体来说比较简单，似乎只是根据内容然后放到相关通道让等待的协议实例继续进行
7. ./announcer: 提交共识结果
8. ./messenger: 
    * 似乎是一个节点用于处理各种收到信息的方法，是网络层建设代码
    * peer: peer描述了节点最外层的接受和发送信息接口，接收信息会根据类型交给不同的handler模块处理，发送信息则比较简单， 另外这里也描述了节点间互相建立连接(注册连接并rpc对方listen)以及连接建立后测试连接状态的方法
    * peer connection: 描述了可选的连接类型
    * client: 描述了服务端提供的grpc服务，如Request，Buckets。这里还定义了让客户与所有orderer建立连接的方法，但是建立后可以使用的grpc服务没有在这里定义

## 脚本的运行流程
* 首先运行 install-local.sh  用以安装相关的工具包 只需要运行一次
* 运行 ./deploy.sh local new scripts/experiment-configuration/generate-local-config.sh
    * 首先执行 scripts/global-vars.sh ： 主要设置了诸如输入输出文件，输出地址，私钥配置，机器端口等信息
    * 其次执行 deploy-xxx.sh: 根据命令部署类型运行不同的部署脚本
        * 以本地部署为例： 
        * 脚本首先去执行run-protoc.sh生成protobuff文件
        * 之后脚本将相关证书文件和指令文件复制到相关地址
        * 然后运行master和slave：
            * master通过discoverymaster启动
            * slave通过脚本 deploy-slaves-local.sh启动
                * 其主要是根据配置生成的实验任务循环的不断配置各种参数然后启动discoveryslave
                * 注意tls设置为false，设置为true在单机有问题
    * 最后执行 analyze/summarize.sh ： 一个结果分析脚本


## 入手位置
1. request的AddReqMsg函数是Req进入系统的步骤，从这里修改处理Req的逻辑，添加Mempool并进行PAB广播
2. 先去设计Mempool和Pab广播 书写测试桩  记住对好接口 即CutBatch输出ReqBatch以及AddReqMsg加入Req，CommitEntry之前进行FillBatch
3. 主要在于AddReqMsg需要经过桶然后进行广播或者回复Ack；CutBatch能够从桶中取出不带负载的Batch
4. mempool主要需要实现的接口是ReceiveTxn,ShareTxn,MakeProposal
    * ReceiveTxn，应该能够将Txn的负载存储下来并产生吸相应的MB
    * ShareTxn，应该能够将该Txn的负载连同MB广播出去(只有桶对应映射为空且请求来自客户时)
    * MakeProposal 则能够制造出用于共识的提议信息
    * FillProposal 对于一个Proposal，其能够根据Proposal的MB获取原来的负载以拼装成logEntry
5. 一共有四种类型的server，contention test类型，discovery 类型，messenger 类型，test service类型
6. PAB广播的是什么? PAB应当立即广播的是用户的Request，而节点的Connections应当能够Send Receive ClientRequest，为了方便，我们将其包装成一种ProtoclMessenage文件




## 更改Batch
1. CutBatch的同时将被加入Batch的Request从Bucket中移除了，（存在CutBatch时节点并不清楚自己的MB吗？ 不会Ack只会发给MBSender，所以可以尝试在这时候就remove req ），所以需要在FillProposal时也要及时移除
2. CutBatch时是均匀的从其所在的BucketGroup中切出Request的，我我们也应该均匀的从BucketGroup选出要处理的Bucket的Pool（根据stable长度选一个可以吗？）
3. CutBatch时等待的是就绪的Req，更改后需要改成就绪的StableMb


## 目前问题
1. 一个Batch下的MB不足 （姑且不算问题）
2. FillBatch问题
